{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c72c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "script_dir = os.path.dirname(Path())\n",
    "data_snippets = json.load(open(os.path.join(script_dir, \"data/snippets.txt\")), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38a84898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "for idx, arguments in enumerate(data_snippets):\n",
    "    arguments['index'] = idx\n",
    "    # print(arguments)\n",
    "\n",
    "# removing arguments with sentences less then 3\n",
    "print(len(data_snippets))\n",
    "count = 0\n",
    "data_snippets_filtered = []\n",
    "for argument_x in data_snippets:\n",
    "\n",
    "    if len(argument_x['sentences']) >= 3:\n",
    "        data_snippets_filtered.append(argument_x)\n",
    "        count = count + 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2275cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48bf5443",
   "metadata": {},
   "outputs": [],
   "source": [
    "argument_query_list =[]\n",
    "for arguments in data_snippets_filtered:\n",
    "    if arguments['query'] not in argument_query_list:\n",
    "        argument_query_list.append(arguments['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc065b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(argument_query_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d275e8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abortion', 'brexit', 'climate change', 'death_penalty', 'donald trump', 'feminism', 'google', 'nuclear_energy', 'trump', 'vegan']\n"
     ]
    }
   ],
   "source": [
    "print(argument_query_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "383a3f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments_query_count =  {}\n",
    "for query in argument_query_list:\n",
    "    \n",
    "    arguments_count = 0\n",
    "    for argument in data_snippets_filtered: \n",
    "        if argument['query']== query:\n",
    "            arguments_count = arguments_count +1\n",
    "        \n",
    "        arguments_query_count[query]= arguments_count\n",
    "    \n",
    "argument_count_list=arguments_query_count  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "658b9fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abortion': 10, 'brexit': 9, 'climate change': 6, 'death_penalty': 9, 'donald trump': 10, 'feminism': 9, 'google': 8, 'nuclear_energy': 3, 'trump': 5, 'vegan': 8}\n"
     ]
    }
   ],
   "source": [
    "print(argument_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83d612eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "940f57c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_args_set = ['feminism','death_penalty', 'brexit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4e97b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_args =[]\n",
    "dev_args  =[]\n",
    "for args in data_snippets_filtered:\n",
    "    if args['query'] in dev_args_set:\n",
    "        dev_args.append(args)\n",
    "    else:\n",
    "        test_args.append(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df03e45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0928328b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0127df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4130749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "INFO:absl:Using C:\\Users\\harsh\\AppData\\Local\\Temp\\tfhub_modules to cache modules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.03133  -0.063386 -0.016075 -0.010349 ...  0.064889 -0.032428 -0.045757  0.053705]\n",
      " [ 0.050809 -0.016524  0.015738 -0.042864 ... -0.01628   0.009767  0.031701  0.017881]\n",
      " [ 0.053329 -0.017461  0.053533  0.057924 ... -0.007853 -0.022668 -0.040348 -0.035826]], shape=(3, 512), dtype=float32)\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"chkla/roberta-argument\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NON-ARGUMENT\",\n",
      "    \"1\": \"ARGUMENT\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"ARGUMENT\": 1,\n",
      "    \"NON-ARGUMENT\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNLearner(data=TextClasDataBunch;\n",
      "\n",
      "Train: LabelList (0 items)\n",
      "x: TextList\n",
      "\n",
      "y: CategoryList\n",
      "\n",
      "Path: ..\\pretrained_models2;\n",
      "\n",
      "Valid: LabelList (0 items)\n",
      "x: TextList\n",
      "\n",
      "y: CategoryList\n",
      "\n",
      "Path: ..\\pretrained_models2;\n",
      "\n",
      "Test: None, model=SequentialRNN(\n",
      "  (0): MultiBatchEncoder(\n",
      "    (module): AWD_LSTM(\n",
      "      (encoder): Embedding(60000, 400, padding_idx=1)\n",
      "      (encoder_dp): EmbeddingDropout(\n",
      "        (emb): Embedding(60000, 400, padding_idx=1)\n",
      "      )\n",
      "      (rnns): ModuleList(\n",
      "        (0): WeightDropout(\n",
      "          (module): LSTM(400, 1152, batch_first=True)\n",
      "        )\n",
      "        (1): WeightDropout(\n",
      "          (module): LSTM(1152, 1152, batch_first=True)\n",
      "        )\n",
      "        (2): WeightDropout(\n",
      "          (module): LSTM(1152, 400, batch_first=True)\n",
      "        )\n",
      "      )\n",
      "      (input_dp): RNNDropout()\n",
      "      (hidden_dps): ModuleList(\n",
      "        (0): RNNDropout()\n",
      "        (1): RNNDropout()\n",
      "        (2): RNNDropout()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): PoolingLinearClassifier(\n",
      "    (layers): Sequential(\n",
      "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Dropout(p=0.27999999999999997, inplace=False)\n",
      "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x000002900B19E558>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=WindowsPath('../pretrained_models2'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
      "learn: ...\n",
      "alpha: 2.0\n",
      "beta: 1.0], layer_groups=[Sequential(\n",
      "  (0): Embedding(60000, 400, padding_idx=1)\n",
      "  (1): Embedding(60000, 400, padding_idx=1)\n",
      "  (2): LSTM(400, 1152, batch_first=True)\n",
      "  (3): LSTM(1152, 1152, batch_first=True)\n",
      "  (4): LSTM(1152, 400, batch_first=True)\n",
      "  (5): RNNDropout()\n",
      "  (6): RNNDropout()\n",
      "  (7): RNNDropout()\n",
      "  (8): RNNDropout()\n",
      "  (9): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (10): Dropout(p=0.27999999999999997, inplace=False)\n",
      "  (11): Linear(in_features=1200, out_features=50, bias=True)\n",
      "  (12): ReLU(inplace=True)\n",
      "  (13): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (14): Dropout(p=0.1, inplace=False)\n",
      "  (15): Linear(in_features=50, out_features=2, bias=True)\n",
      ")], add_time=True, silent=False)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import tensorflow_hub as hub\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from fastai.text.learner import load_learner\n",
    "\n",
    "nltk.download('punkt')\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "embeddings = embed([\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"I am a sentence for which I would like to get its embedding\", \"Agree?\"])\n",
    "\n",
    "print(embeddings)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"chkla/roberta-argument\")\n",
    "arg_model = AutoModelForSequenceClassification.from_pretrained(\"chkla/roberta-argument\")\n",
    "print(arg_model.config)\n",
    "model_path = \"../pretrained_models2\"\n",
    "claim_classifier = load_learner(model_path)\n",
    "print(claim_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "720195f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snippetGenerator import SnippetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "791b6e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(arguments, d, mc_method, aspects_arguments_max, aspects_weights,\n",
    "               argument_context, argumentative_score_method):\n",
    "    snippetGenerator= SnippetGenerator(arguments, d, mc_method, aspects_arguments_max, aspects_weights,\n",
    "                                     argument_context, argumentative_score_method)\n",
    "    snippets = snippetGenerator.get_snippets(arguments)\n",
    "    count, accuracy= snippetGenerator.get_accuracy(arguments, snippets)\n",
    "    return count,accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9266b8e5",
   "metadata": {},
   "source": [
    "# Test Snippet Generation using different parameters --"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5410dc",
   "metadata": {},
   "source": [
    "1. Previous Vesion of the snippet Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4bfc8d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context_array [0, 1, 0]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 0.15\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 0\n",
      "snippets generated File created \n",
      "18.5\n",
      "Accuracy  37.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18.5, 37.0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# methodSet = ['power','eigen','linear','krylov']\n",
    "argumentative_score_methods = ['discourse_claim_markers', 'argument_score', 'claim_score', 'hybrid_score']\n",
    "d = 0.15\n",
    "mc_method = 'linear'\n",
    "aspects_arguments_max = 0\n",
    "aspects_weights = [0, 0]\n",
    "arguments = test_args\n",
    "# argument_context =[1,1,1]\n",
    "# argument_context_clusters = ['query',same page','aspect']\n",
    "argument_context = [0, 1, 0]\n",
    "argumentative_score_method = argumentative_score_methods[0]\n",
    "\n",
    "evaluation(arguments, d, mc_method, aspects_arguments_max, aspects_weights,\n",
    "               argument_context, argumentative_score_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a0baa8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting d= 0 for evaluation of argument context modelling task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "46c6448a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context_array [0, 1, 0]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 0\n",
      "snippets generated File created \n",
      "18.5\n",
      "Accuracy  37.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18.5, 37.0)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# methodSet = ['power','eigen','linear','krylov']\n",
    "argumentative_score_methods = ['discourse_claim_markers', 'argument_score', 'claim_score', 'hybrid_score']\n",
    "d = 0\n",
    "mc_method = 'linear'\n",
    "aspects_arguments_max = 0\n",
    "aspects_weights = [0, 0]\n",
    "arguments = test_args\n",
    "# argument_context =[1,1,1]\n",
    "# argument_context_clusters = ['query',same page','aspect']\n",
    "argument_context = [0, 1, 0]\n",
    "argumentative_score_method = argumentative_score_methods[0]\n",
    "\n",
    "evaluation(arguments, d, mc_method, aspects_arguments_max, aspects_weights,\n",
    "               argument_context, argumentative_score_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2564c5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context_array [1, 1, 0]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 0\n",
      "snippets generated File created \n",
      "19.5\n",
      "Accuracy  39.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19.5, 39.0)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# methodSet = ['power','eigen','linear','krylov']\n",
    "argumentative_score_methods = ['discourse_claim_markers', 'argument_score', 'claim_score', 'hybrid_score']\n",
    "d = 0\n",
    "mc_method = 'linear'\n",
    "aspects_arguments_max = 0\n",
    "aspects_weights = [0, 0]\n",
    "arguments = test_args\n",
    "# argument_context =[1,1,1]\n",
    "# argument_context_clusters = ['query',same page','aspect']\n",
    "argument_context = [1, 1, 0]\n",
    "argumentative_score_method = argumentative_score_methods[0]\n",
    "\n",
    "evaluation(arguments, d, mc_method, aspects_arguments_max, aspects_weights,\n",
    "               argument_context, argumentative_score_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "78fd24b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context_array [1, 0, 1]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 50\n",
      "snippets generated File created \n",
      "21.0\n",
      "Accuracy  42.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(21.0, 42.0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# methodSet = ['power','eigen','linear','krylov']\n",
    "argumentative_score_methods = ['discourse_claim_markers', 'argument_score', 'claim_score', 'hybrid_score']\n",
    "d = 0\n",
    "mc_method = 'linear'\n",
    "aspects_arguments_max = 50\n",
    "aspects_weights = [0, 0]\n",
    "arguments = test_args\n",
    "# argument_context =[1,1,1]\n",
    "# argument_context_clusters = ['query',same page','aspect']\n",
    "argument_context = [1, 0, 1]\n",
    "argumentative_score_method = argumentative_score_methods[0]\n",
    "\n",
    "evaluation(arguments, d, mc_method, aspects_arguments_max, aspects_weights,\n",
    "               argument_context, argumentative_score_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c3d99ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context_array [1, 1, 1]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 50\n",
      "snippets generated File created \n",
      "20.5\n",
      "Accuracy  41.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20.5, 41.0)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# methodSet = ['power','eigen','linear','krylov']\n",
    "argumentative_score_methods = ['discourse_claim_markers', 'argument_score', 'claim_score', 'hybrid_score']\n",
    "d = 0\n",
    "mc_method = 'linear'\n",
    "aspects_arguments_max = 50\n",
    "aspects_weights = [0, 0]\n",
    "arguments = test_args\n",
    "# argument_context =[1,1,1]\n",
    "# argument_context_clusters = ['query',same page','aspect']\n",
    "argument_context = [1, 1, 1]\n",
    "argumentative_score_method = argumentative_score_methods[0]\n",
    "\n",
    "evaluation(arguments, d, mc_method, aspects_arguments_max, aspects_weights,\n",
    "               argument_context, argumentative_score_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f2a952ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context_array [1, 1, 1]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 100\n",
      "snippets generated File created \n",
      "19.5\n",
      "Accuracy  39.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19.5, 39.0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# methodSet = ['power','eigen','linear','krylov']\n",
    "argumentative_score_methods = ['discourse_claim_markers', 'argument_score', 'claim_score', 'hybrid_score']\n",
    "d = 0\n",
    "mc_method = 'linear'\n",
    "aspects_arguments_max = 100\n",
    "aspects_weights = [0, 0]\n",
    "arguments = test_args\n",
    "# argument_context =[1,1,1]\n",
    "# argument_context_clusters = ['query',same page','aspect']\n",
    "argument_context = [1, 1, 1]\n",
    "argumentative_score_method = argumentative_score_methods[0]\n",
    "\n",
    "evaluation(arguments, d, mc_method, aspects_arguments_max, aspects_weights,\n",
    "               argument_context, argumentative_score_method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a33960",
   "metadata": {},
   "source": [
    "## 2.Evaluation on  number of aspect generated arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f730cd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context_array [1, 0, 1]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 0\n",
      "snippets generated File created \n",
      "19.5\n",
      "Accuracy  39.0\n",
      "Context_array [1, 0, 1]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 50\n",
      "snippets generated File created \n",
      "21.0\n",
      "Accuracy  42.0\n",
      "Context_array [1, 0, 1]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 100\n",
      "snippets generated File created \n",
      "21.0\n",
      "Accuracy  42.0\n",
      "Context_array [1, 0, 1]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 150\n",
      "snippets generated File created \n",
      "22.0\n",
      "Accuracy  44.0\n",
      "Context_array [1, 0, 1]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 200\n",
      "snippets generated File created \n",
      "22.5\n",
      "Accuracy  45.0\n",
      "Context_array [1, 0, 1]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 250\n",
      "snippets generated File created \n",
      "21.5\n",
      "Accuracy  43.0\n",
      "Context_array [1, 0, 1]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 300\n",
      "snippets generated File created \n",
      "21.0\n",
      "Accuracy  42.0\n"
     ]
    }
   ],
   "source": [
    "d = 0\n",
    "# methodSet = ['power','eigen','linear','krylov']\n",
    "mc_method = 'linear'\n",
    "argumentative_score_methods = ['discourse_claim_markers', 'argument_score', 'claim_score', 'hybrid_score']\n",
    "count_list =[]\n",
    "accuracy_list =[]\n",
    "\n",
    "aspects_weights = [0, 0]\n",
    "arguments = test_args\n",
    "aspects_arguments_max_list = [0,50,100,150,200,250,300]\n",
    "argumentative_score_method = argumentative_score_methods[0]\n",
    "argument_context =[1,0,1]\n",
    "\n",
    "accuracy_list_aspect = []\n",
    "count_list_aspect =[]\n",
    "\n",
    "\n",
    "for aspects_arguments_max in aspects_arguments_max_list:\n",
    "    count,accuracy =evaluation(arguments, d, mc_method, aspects_arguments_max, aspects_weights,\n",
    "               argument_context, argumentative_score_method)\n",
    "    accuracy_list_aspect.append(accuracy)\n",
    "    count_list_aspect.append(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "023b78e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "evaluation_aspect_detection_test = pd.DataFrame(\n",
    "    {'Number of Arguemts': aspects_arguments_max_list,\n",
    "     'match count': accuracy_list_aspect,\n",
    "     'accuracy (%)': count_list_aspect\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fa0e5b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Arguemts</th>\n",
       "      <th>match count</th>\n",
       "      <th>accuracy (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19.5</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>21.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>21.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>150</td>\n",
       "      <td>22.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>22.5</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>250</td>\n",
       "      <td>21.5</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>300</td>\n",
       "      <td>21.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of Arguemts  match count  accuracy (%)\n",
       "0                   0         19.5          39.0\n",
       "1                  50         21.0          42.0\n",
       "2                 100         21.0          42.0\n",
       "3                 150         22.0          44.0\n",
       "4                 200         22.5          45.0\n",
       "5                 250         21.5          43.0\n",
       "6                 300         21.0          42.0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_aspect_detection_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f4366a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_aspect_detection_test.to_csv('data/evaluation_aspect_detection_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec5de761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context_array [1, 0, 1]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 0\n",
      "WARNING:tensorflow:5 out of the last 54 calls to <function recreate_function.<locals>.restored_function_body at 0x0000017315252168> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 54 calls to <function recreate_function.<locals>.restored_function_body at 0x0000017315252168> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 55 calls to <function recreate_function.<locals>.restored_function_body at 0x00000173E6B57C18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 55 calls to <function recreate_function.<locals>.restored_function_body at 0x00000173E6B57C18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snippets generated File created \n",
      "8.5\n",
      "Accuracy  31.48148148148148\n",
      "Context_array [1, 0, 1]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 50\n",
      "snippets generated File created \n",
      "9.5\n",
      "Accuracy  35.18518518518518\n",
      "Context_array [1, 0, 1]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 100\n",
      "snippets generated File created \n",
      "10.5\n",
      "Accuracy  38.888888888888886\n",
      "Context_array [1, 0, 1]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 150\n",
      "snippets generated File created \n",
      "12.0\n",
      "Accuracy  44.44444444444444\n",
      "Context_array [1, 0, 1]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 200\n",
      "snippets generated File created \n",
      "12.5\n",
      "Accuracy  46.2962962962963\n",
      "Context_array [1, 0, 1]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 250\n",
      "snippets generated File created \n",
      "11.5\n",
      "Accuracy  42.592592592592595\n",
      "Context_array [1, 0, 1]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 300\n",
      "snippets generated File created \n",
      "11.5\n",
      "Accuracy  42.592592592592595\n"
     ]
    }
   ],
   "source": [
    "d = 0\n",
    "# methodSet = ['power','eigen','linear','krylov']\n",
    "mc_method = 'linear'\n",
    "argumentative_score_methods = ['discourse_claim_markers', 'argument_score', 'claim_score', 'hybrid_score']\n",
    "count_list =[]\n",
    "accuracy_list =[]\n",
    "\n",
    "aspects_weights = [0, 0]\n",
    "arguments = dev_args\n",
    "# aspects_arguments_max_list = [0,50,100,150,200,250,300]\n",
    "argumentative_score_method = argumentative_score_methods[0]\n",
    "argument_context =[1,0,1]\n",
    "\n",
    "accuracy_list_aspect = []\n",
    "count_list_aspect =[]\n",
    "\n",
    "\n",
    "for aspects_arguments_max in aspects_arguments_max_list:\n",
    "    accuracy, count =evaluation(arguments, d, mc_method, aspects_arguments_max, aspects_weights,\n",
    "               argument_context, argumentative_score_method)\n",
    "    accuracy_list_aspect.append(accuracy)\n",
    "    count_list_aspect.append(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119640d",
   "metadata": {},
   "source": [
    "## Selecting d=1 to test argumentatative score method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a1227022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context_array [0, 0, 0]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 0\n",
      "snippets generated File created \n",
      "17.5\n",
      "Accuracy  35.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17.5, 35.0)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# methodSet = ['power','eigen','linear','krylov']\n",
    "argumentative_score_methods = ['discourse_claim_markers', 'argument_score', 'claim_score', 'hybrid_score']\n",
    "d = 1\n",
    "mc_method = 'linear'\n",
    "aspects_arguments_max = 0\n",
    "aspects_weights = [0, 0]\n",
    "arguments = test_args\n",
    "# argument_context =[1,1,1]\n",
    "# argument_context_clusters = ['query',same page','aspect']\n",
    "argument_context = [0, 0, 0]\n",
    "argumentative_score_method = argumentative_score_methods[0]\n",
    "\n",
    "evaluation(arguments, d, mc_method, aspects_arguments_max, aspects_weights,\n",
    "               argument_context, argumentative_score_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a074d8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context_array [0, 0, 0]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 0\n",
      "snippets generated File created \n",
      "17.5\n",
      "Accuracy  35.0\n",
      "Context_array [0, 0, 0]\n",
      "Markov Chain Method linear Argumentative Score Method argument_score d: 1\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x00000290DEE011F8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x00000290DEE011F8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 0\n",
      "snippets generated File created \n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x00000290DEE014C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x00000290DEE014C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.0\n",
      "Accuracy  38.0\n",
      "Context_array [0, 0, 0]\n",
      "Markov Chain Method linear Argumentative Score Method claim_score d: 1\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function recreate_function.<locals>.restored_function_body at 0x00000290DD7BB048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 7 calls to <function recreate_function.<locals>.restored_function_body at 0x00000290DD7BB048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 0\n",
      "snippets generated File created \n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function recreate_function.<locals>.restored_function_body at 0x00000290DD9493A8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:8 out of the last 8 calls to <function recreate_function.<locals>.restored_function_body at 0x00000290DD9493A8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.5\n",
      "Accuracy  29.0\n",
      "Context_array [0, 0, 0]\n",
      "Markov Chain Method linear Argumentative Score Method hybrid_score d: 1\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function recreate_function.<locals>.restored_function_body at 0x00000290DD7A6CA8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:9 out of the last 9 calls to <function recreate_function.<locals>.restored_function_body at 0x00000290DD7A6CA8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 0\n",
      "snippets generated File created \n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function recreate_function.<locals>.restored_function_body at 0x00000290DD9641F8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:10 out of the last 10 calls to <function recreate_function.<locals>.restored_function_body at 0x00000290DD9641F8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.5\n",
      "Accuracy  31.0\n"
     ]
    }
   ],
   "source": [
    "accuracy_list_args_method = []\n",
    "count_list_args_method = []\n",
    "\n",
    "# methodSet = ['power','eigen','linear','krylov']\n",
    "argumentative_score_methods = ['discourse_claim_markers', 'argument_score', 'claim_score', 'hybrid_score']\n",
    "d = 1\n",
    "mc_method = 'linear'\n",
    "aspects_arguments_max = 0\n",
    "aspects_weights = [0, 0]\n",
    "arguments = test_args\n",
    "# argument_context =[1,1,1]\n",
    "# argument_context_clusters = ['query',same page','aspect']\n",
    "argument_context = [0, 0, 0]\n",
    "#argumentative_score_method = argumentative_score_methods[0]\n",
    "for argumentative_score_method in argumentative_score_methods:\n",
    "    count, accuracy =evaluation(arguments, d, mc_method, aspects_arguments_max, aspects_weights,\n",
    "               argument_context, argumentative_score_method)\n",
    "    accuracy_list_args_method.append(accuracy)\n",
    "    count_list_args_method.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4889958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "evaluation_argumentation_methods_test = pd.DataFrame(\n",
    "    {'Argumentataion-score-methods': argumentative_score_methods,\n",
    "     'match count': accuracy_list_args_method,\n",
    "     'accuracy (%)': count_list_args_method\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15cc01b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Argumentataion-score-methods</th>\n",
       "      <th>match count</th>\n",
       "      <th>accuracy (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>discourse_claim_markers</td>\n",
       "      <td>35.0</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>argument_score</td>\n",
       "      <td>38.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>claim_score</td>\n",
       "      <td>29.0</td>\n",
       "      <td>14.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hybrid_score</td>\n",
       "      <td>31.0</td>\n",
       "      <td>15.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Argumentataion-score-methods  match count  accuracy (%)\n",
       "0      discourse_claim_markers         35.0          17.5\n",
       "1               argument_score         38.0          19.0\n",
       "2                  claim_score         29.0          14.5\n",
       "3                 hybrid_score         31.0          15.5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_argumentation_methods_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2af41ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context_array [1, 0, 1]\n",
      "Markov Chain Method linear Argumentative Score Method discourse_claim_markers d: 1\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000290A67F5B88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000290A67F5B88> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 200\n",
      "snippets generated File created \n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000290A67E15E8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000290A67E15E8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n",
      "Accuracy  32.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16.0, 32.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_list_args_method = []\n",
    "count_list_args_method = []\n",
    "\n",
    "# methodSet = ['power','eigen','linear','krylov']\n",
    "argumentative_score_methods = ['discourse_claim_markers', 'argument_score', 'claim_score', 'hybrid_score']\n",
    "d = 1\n",
    "mc_method = 'linear'\n",
    "aspects_arguments_max = 200\n",
    "aspects_weights = [0, 0]\n",
    "arguments = test_args\n",
    "# argument_context =[1,1,1]\n",
    "# argument_context_clusters = ['query',same page','aspect']\n",
    "argument_context = [1, 0, 1]\n",
    "argumentative_score_method =argumentative_score_methods[0]\n",
    "evaluation(arguments, d, mc_method, aspects_arguments_max, aspects_weights,\n",
    "               argument_context, argumentative_score_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc85d3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context_array [1, 0, 1]\n",
      "Markov Chain Method linear Argumentative Score Method argument_score d: 1\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000290A2A054C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000290A2A054C8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 200\n",
      "snippets generated File created \n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000290A2059D38> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x00000290A2059D38> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.0\n",
      "Accuracy  38.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19.0, 38.0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_list_args_method = []\n",
    "count_list_args_method = []\n",
    "\n",
    "# methodSet = ['power','eigen','linear','krylov']\n",
    "argumentative_score_methods = ['discourse_claim_markers', 'argument_score', 'claim_score', 'hybrid_score']\n",
    "d = 1\n",
    "mc_method = 'linear'\n",
    "aspects_arguments_max = 200\n",
    "aspects_weights = [0, 0]\n",
    "arguments = test_args\n",
    "# argument_context =[1,1,1]\n",
    "# argument_context_clusters = ['query',same page','aspect']\n",
    "argument_context = [1, 0, 1]\n",
    "argumentative_score_method =argumentative_score_methods[1]\n",
    "evaluation(arguments, d, mc_method, aspects_arguments_max, aspects_weights,\n",
    "               argument_context, argumentative_score_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba187854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context_array [1, 0, 1]\n",
      "Markov Chain Method linear Argumentative Score Method claim_score d: 1\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x000002909B7FB0D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x000002909B7FB0D8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspects_weights: [0, 0] aspects_arguments_max 200\n"
     ]
    }
   ],
   "source": [
    "accuracy_list_args_method = []\n",
    "count_list_args_method = []\n",
    "\n",
    "# methodSet = ['power','eigen','linear','krylov']\n",
    "argumentative_score_methods = ['discourse_claim_markers', 'argument_score', 'claim_score', 'hybrid_score']\n",
    "d = 1\n",
    "mc_method = 'linear'\n",
    "aspects_arguments_max = 200\n",
    "aspects_weights = [0, 0]\n",
    "arguments = test_args\n",
    "# argument_context =[1,1,1]\n",
    "# argument_context_clusters = ['query',same page','aspect']\n",
    "argument_context = [1, 0, 1]\n",
    "argumentative_score_method =argumentative_score_methods[2]\n",
    "evaluation(arguments, d, mc_method, aspects_arguments_max, aspects_weights,\n",
    "               argument_context, argumentative_score_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b110ac4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
